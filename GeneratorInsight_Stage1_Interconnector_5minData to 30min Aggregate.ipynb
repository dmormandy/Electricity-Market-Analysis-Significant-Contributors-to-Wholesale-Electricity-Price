{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Main Contributers to Electricity RRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Data Processing to produce Dataset of 30 min Average of 5min Interconnector Dispatch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "# I've included a handful of libraries here that might be useful\n",
    "import glob\n",
    "import os, zipfile, shutil\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import display # Used to display multiple pandas tables in one cell\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from scipy import stats\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dispatch Data for Interconnector Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source Data is AEMO Dispatch Data from http://www.nemweb.com.au/REPORTS/ARCHIVE/DispatchIS_Reports/.\n",
    "One month at a time was analysed for this project.\n",
    "\n",
    "This file contains public 5 minute dispatch data by region. Data covers interconnector flows, constraints, \n",
    "regional reference price, demand, dispatchable generation, dispatchable load, and ancillary services data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the path to the data files - change this to where you store your files\n",
    "from_dir = 'F:\\\\RMIT_MasterofAnalytics\\\\LEP_DataScience\\\\Assignments\\\\Datasets\\\\DISPATCHIS_Nov16'\n",
    "to_dir = 'F:\\\\RMIT_MasterofAnalytics\\\\LEP_DataScience\\\\Assignments\\\\Datasets\\\\DISPATCHIS_Unzipped'\n",
    "extension = \".zip\"\n",
    "\n",
    "os.chdir(from_dir) # change directory from working dir to dir with files\n",
    "\n",
    "for item in os.listdir(from_dir): # loop through items in dir\n",
    "    if item.endswith(extension): # check for \".zip\" extension\n",
    "        file_name = os.path.abspath(item) # get full path of files\n",
    "        zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "        zip_ref.extractall(to_dir) # extract file to dir\n",
    "        zip_ref.close() # close file\n",
    "        os.remove(file_name) # delete zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the path to the data files - change this to where you store your files\n",
    "from_dir = 'F:\\\\RMIT_MasterofAnalytics\\\\LEP_DataScience\\\\Assignments\\\\Datasets\\\\DISPATCHIS_Unzipped'\n",
    "to_dir = 'F:\\\\RMIT_MasterofAnalytics\\\\LEP_DataScience\\\\Assignments\\\\Datasets\\\\DISPATCHIS_Unzipcsv'\n",
    "extension = \".zip\"\n",
    "\n",
    "os.chdir(from_dir) # change directory from working dir to dir with files\n",
    "\n",
    "for item in os.listdir(from_dir): # loop through items in dir\n",
    "    if item.endswith(extension): # check for \".zip\" extension\n",
    "        file_name = os.path.abspath(item) # get full path of files\n",
    "        zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "        zip_ref.extractall(to_dir) # extract file to dir\n",
    "        zip_ref.close() # close file\n",
    "        os.remove(file_name) # delete zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analysisPath = 'F:/RMIT_MasterofAnalytics/LEP_DataScience/Assignments/Datasets/DISPATCHIS_Unzipcsv/'\n",
    "count = len([f for f in os.listdir(analysisPath) if os.path.isfile(os.path.join(analysisPath, f))])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change this if necessary\n",
    "numfiles = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the path to the data files - change this to where you store your files\n",
    "from_dir = 'F:\\\\RMIT_MasterofAnalytics\\\\LEP_DataScience\\\\Assignments\\\\Datasets\\\\DISPATCHIS_Unzipcsv'\n",
    "to_dir = 'F:\\\\RMIT_MasterofAnalytics\\\\LEP_DataScience\\\\Assignments\\\\Datasets\\\\DISPATCHIS_Processed'\n",
    "extension = \".CSV\"\n",
    "\n",
    "os.chdir(from_dir) # change directory from working dir to dir with files\n",
    "count=0\n",
    "df_temp = pd.DataFrame()\n",
    "for i in range(16,150):\n",
    "    print(i)\n",
    "    for file in os.listdir(from_dir): # loop through items in dir\n",
    "        if file.endswith(extension): # check for \".zip\" extension\n",
    "            if (count<2000):\n",
    "                if count % 100 == 0:\n",
    "                    print(count)\n",
    "                input_file = os.path.join(from_dir, file)\n",
    "                dest_file = os.path.join(to_dir, file)\n",
    "                # Read the csv files into dataframes\n",
    "                df_input = pd.read_csv(input_file,sep=',',skiprows=i,nrows=6,error_bad_lines=False,warn_bad_lines=False)\n",
    "                if (df_input.columns[0] == 'I') & (df_input.columns[2] == 'INTERCONNECTORRES'):\n",
    "                    df_temp = pd.concat([df_temp, df_input], axis=0)\n",
    "                    count = count + 1\n",
    "                    # Move file once processed\n",
    "                    shutil.move(input_file, dest_file)\n",
    "            \n",
    "numfiles = numfiles + 1\n",
    "outputfile = 'F:/RMIT_MasterofAnalytics/LEP_DataScience/Assignments/Datasets/DISPATCHIS_DataFrame/DISPATCHIS_DataFrameNov16_' + str(numfiles) + '.csv'\n",
    "df_temp.to_csv(outputfile,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysisPath = 'F:/RMIT_MasterofAnalytics/LEP_DataScience/Assignments/Datasets/DISPATCHIS_DataFrame/Nov16/'\n",
    "df_final = pd.DataFrame()\n",
    "for file in os.listdir(analysisPath):\n",
    "    if file.endswith(\".csv\"):\n",
    "        input_file = os.path.join(analysisPath, file)\n",
    "        # Read the csv files into dataframes\n",
    "        df_input = pd.read_csv(input_file,sep=',',skiprows=0,error_bad_lines=False,warn_bad_lines=False)\n",
    "        df_final = pd.concat([df_final, df_input], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_final.shape)\n",
    "print(df_final.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert 'SETTLEMENTDATE' to datetime field\n",
    "df_final['SETTLEMENTDATE'] = pd.to_datetime(df_final['SETTLEMENTDATE'], errors='coerce')\n",
    "df_final = df_final.dropna(subset=['SETTLEMENTDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subtract one minute to allow masking into 30 minute aggregates\n",
    "df_final['SETTLEMENTDATE'] = df_final['SETTLEMENTDATE'] - timedelta(minutes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Date and Time Fields\n",
    "df_final['DATE'] = df_final['SETTLEMENTDATE'].dt.date\n",
    "df_final['HOUR'] = df_final['SETTLEMENTDATE'].dt.hour\n",
    "df_final['MINUTE'] = df_final['SETTLEMENTDATE'].dt.minute\n",
    "print(df_final['MINUTE'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Masks created to calculate 30minute averages\n",
    "mask1 = (df_final['MINUTE'] >  0.0 ) & (df_final['MINUTE'] <=  30.0 )\n",
    "mask2 = (df_final['MINUTE'] >  30.0 ) & (df_final['MINUTE'] <=  59.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create groups for 30 minute averaging\n",
    "df_Group1 = df_final.loc[mask1]\n",
    "df_Group2 = df_final.loc[mask2]\n",
    "print(df_Group1.tail(5))\n",
    "print(df_Group2.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Unnecessary columns\n",
    "df_final.drop(['IMPORTGENCONID','DISPATCHINTERVAL','I','RUNNO','SETTLEMENTDATE','DISPATCH'], axis=1,inplace=True)\n",
    "df_final.drop(['LOCALLY_CONSTRAINED_IMPORT','LOCAL_PRICE_ADJUSTMENT_IMPORT','LOCALLY_CONSTRAINED_EXPORT','LOCAL_PRICE_ADJUSTMENT_EXPORT','FCASIMPORTLIMIT','FCASEXPORTLIMIT'], axis=1,inplace=True)\n",
    "df_final.drop(['EXPORTGENCONID','MARGINALLOSS','INTERCONNECTORRES','INTERVENTION','3'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 30 minute Averages\n",
    "df_30min_Group1 = df_final.loc[mask1].groupby(['INTERCONNECTORID','HOUR','DATE'],as_index=False)['MWFLOW'].mean()\n",
    "df_30min_Group1['MINUTE'] = 30.0\n",
    "print(df_30min_Group1.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 30 minute Averages\n",
    "df_30min_Group2 = df_final.loc[mask2].groupby(['INTERCONNECTORID','HOUR','DATE'],as_index=False)['MWFLOW'].mean()\n",
    "df_30min_Group2['MINUTE'] = 59.0\n",
    "print(df_30min_Group2.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine 30 minute Averages into one dataset\n",
    "df_INTER30min = pd.concat([df_30min_Group1, df_30min_Group2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_INTER30min['DATE'] = df_INTER30min['DATE'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recreate 'DATETIME' field\n",
    "df_INTER30min['DATETIME'] = df_INTER30min['DATE'] + pd.to_timedelta(df_INTER30min['HOUR'], unit='h') + pd.to_timedelta(df_INTER30min['MINUTE'], unit='m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Need to add one minute to times with minute = 59\n",
    "mask = (df_INTER30min['MINUTE']==59.0)\n",
    "df_INTER30min.loc[mask,'DATETIME']= df_INTER30min.loc[mask,'DATETIME'] + timedelta(minutes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_INTER30min.drop(['DATE','HOUR','MINUTE'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_INTERoutput = df_INTER30min.pivot(index='DATETIME', columns='INTERCONNECTORID', values='MWFLOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_INTERoutput['DATETIME'] = df_INTERoutput.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_INTERoutput.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Dataset \n",
    "outputfile='F:/RMIT_MasterofAnalytics/LEP_DataScience/Assignments/Datasets/DispatchIS_Nov16_30minAverage.csv'\n",
    "df_INTERoutput.to_csv(outputfile,index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
